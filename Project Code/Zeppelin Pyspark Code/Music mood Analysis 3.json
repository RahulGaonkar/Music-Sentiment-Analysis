{"paragraphs":[{"text":"%pyspark\nprint(\"%html <h3>Import<h3>\")","user":"anonymous","dateUpdated":"2018-05-06T03:33:37-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Import<h3>\n"}]},"apps":[],"jobName":"paragraph_1525591517044_196956793","id":"20180503-021205_1733421978","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:33:37-0400","dateFinished":"2018-05-06T03:34:04-0400","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5775"},{"text":"%pyspark\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.ml.feature import *\nfrom pyspark.ml.classification import *\nfrom pyspark.ml.evaluation import * \nfrom pyspark.sql.functions import *\nfrom nltk.stem.porter import *\nfrom pyspark.sql.functions import col, lower, regexp_replace, split\nfrom pyspark.sql.types import *\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder","user":"anonymous","dateUpdated":"2018-05-06T20:17:07-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1525591517044_196956793","id":"20180502-155007_345629139","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T20:17:08-0400","dateFinished":"2018-05-06T20:17:38-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5776"},{"text":"%pyspark\nprint(\"%html <h3>Read CSV<h3>\")","user":"anonymous","dateUpdated":"2018-05-06T03:33:39-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Read CSV<h3>\n"}]},"apps":[],"jobName":"paragraph_1525591517044_196956793","id":"20180503-021235_1722103350","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:34:04-0400","dateFinished":"2018-05-06T03:34:11-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5777"},{"text":"%pyspark\ndf = spark.read.csv(\"train_lyrics_1000.csv\", header='true', multiLine='true', escape=\"\\\"\")\ndf.na.drop()\ndf.count()\n","user":"anonymous","dateUpdated":"2018-05-06T20:17:13-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1000\n"}]},"apps":[],"jobName":"paragraph_1525591517044_196956793","id":"20180502-155046_791978164","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T20:17:13-0400","dateFinished":"2018-05-06T20:17:46-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5778"},{"text":"%pyspark\nprint(\"%html <h3>Chop off Unimportant columns<h3>\")","user":"anonymous","dateUpdated":"2018-05-06T03:33:39-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Chop off Unimportant columns<h3>\n"}]},"apps":[],"jobName":"paragraph_1525591517045_196572044","id":"20180503-021249_1766581863","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:34:11-0400","dateFinished":"2018-05-06T03:34:22-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5779"},{"text":"%pyspark\ndf = df['lyrics','mood']\ndf.printSchema()\n","user":"anonymous","dateUpdated":"2018-05-06T20:17:17-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- lyrics: string (nullable = true)\n |-- mood: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1525591517045_196572044","id":"20180502-155119_1957843056","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T20:17:39-0400","dateFinished":"2018-05-06T20:17:46-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5780"},{"text":"%pyspark\nprint(\"%html <h3>Clean Text<h3>\")","user":"anonymous","dateUpdated":"2018-05-06T03:33:39-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Clean Text<h3>\n"}]},"apps":[],"jobName":"paragraph_1525591517045_196572044","id":"20180503-021319_115937887","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:34:22-0400","dateFinished":"2018-05-06T03:34:22-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5781"},{"text":"%pyspark\n\ndef clean_text(c):\n  c = lower(c)\n  c = regexp_replace(c, \"^rt \", \"\")\n  c = regexp_replace(c, \"(https?\\://)\\S+\", \"\")\n  c = regexp_replace(c, \"[^a-zA-Z0-9\\\\s]\", \"\")\n  return c\n  \nclean_text_df = df.select(clean_text(col(\"lyrics\")).alias(\"words\"), col(\"mood\"))","user":"anonymous","dateUpdated":"2018-05-06T20:17:23-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1525591517045_196572044","id":"20180502-155208_398921512","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T20:17:46-0400","dateFinished":"2018-05-06T20:17:47-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5782"},{"text":"%pyspark\nprint(\"%html <h3>Tokenize<h3>\")","user":"anonymous","dateUpdated":"2018-05-06T03:33:40-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Tokenize<h3>\n"}]},"apps":[],"jobName":"paragraph_1525591517045_196572044","id":"20180503-021331_464577561","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:34:22-0400","dateFinished":"2018-05-06T03:34:22-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5783"},{"text":"%pyspark\ntokenizer = Tokenizer(inputCol=\"words\", outputCol=\"tokens\")\nvector_df = tokenizer.transform(clean_text_df).select(\"tokens\", \"mood\")","user":"anonymous","dateUpdated":"2018-05-06T20:17:31-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1525591517046_197726291","id":"20180502-155729_1874031260","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T20:17:47-0400","dateFinished":"2018-05-06T20:17:47-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5784"},{"text":"%pyspark\nprint(\"%html <h3>Stop Words Removal<h3>\")","user":"anonymous","dateUpdated":"2018-05-06T03:33:40-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Stop Words Removal<h3>\n"}]},"apps":[],"jobName":"paragraph_1525591517046_197726291","id":"20180503-021351_863564655","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:34:22-0400","dateFinished":"2018-05-06T03:34:22-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5785"},{"text":"%pyspark\n# Define a list of stop words or use default list\nremover = StopWordsRemover()\n\n# Specify input/output columns\nremover.setInputCol(\"tokens\")\nremover.setOutputCol(\"vector_no_stopw\")\n\n# Transform existing dataframe with the StopWordsRemover\nvector_no_stopw_df = remover.transform(vector_df).select(\"vector_no_stopw\", \"mood\")","user":"anonymous","dateUpdated":"2018-05-06T20:17:35-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1525591517046_197726291","id":"20180502-160012_1590717162","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T20:17:47-0400","dateFinished":"2018-05-06T20:17:47-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5786"},{"text":"%pyspark\nprint(\"%html <h3>Stemming<h3>\")","user":"anonymous","dateUpdated":"2018-05-06T03:33:40-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Stemming<h3>\n"}]},"apps":[],"jobName":"paragraph_1525591517046_197726291","id":"20180503-021405_1179459922","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:34:22-0400","dateFinished":"2018-05-06T03:34:22-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5787"},{"text":"%pyspark\n# Instantiate stemmer object\nstemmer = PorterStemmer()\n\ndef stem(in_vec):\n    out_vec = []\n    for t in in_vec:\n        t_stem = stemmer.stem(t)\n        if len(t_stem) > 2:\n            out_vec.append(t_stem)       \n    return out_vec\n    \n\nstemmer_udf = udf(lambda x: stem(x), ArrayType(StringType()))\n\n# Create new df with vectors containing the stemmed tokens \nvector_stemmed_df = (\n    vector_no_stopw_df\n        .withColumn(\"vector_stemmed\", stemmer_udf(\"vector_no_stopw\"))\n        .withColumn(\"mood\", vector_no_stopw_df['mood'])\n        .select(\"vector_stemmed\", \"mood\")\n)\n\n# Rename df and column for clarity\nproduction_df1 = vector_stemmed_df.select(col(\"vector_stemmed\").alias(\"unigrams\"), col(\"mood\"))\n","user":"anonymous","dateUpdated":"2018-05-06T20:17:38-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1525591517046_197726291","id":"20180502-160410_1011504829","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T20:17:47-0400","dateFinished":"2018-05-06T20:17:47-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5788"},{"text":"%pyspark\nprint(\"%html <h3>N-Grams<h3>\")","user":"anonymous","dateUpdated":"2018-05-06T03:33:40-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>N-Grams<h3>\n"}]},"apps":[],"jobName":"paragraph_1525591517047_197341542","id":"20180503-021415_1745648701","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:34:22-0400","dateFinished":"2018-05-06T03:34:22-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5789"},{"text":"%pyspark\n# Define NGram transformer\nngram = NGram(n=2, inputCol=\"unigrams\", outputCol=\"bigrams\")\n\n# Create bigram_df as a transform of unigram_df using NGram tranformer\nproduction_df2 = ngram.transform(production_df1)\n","user":"anonymous","dateUpdated":"2018-05-06T20:17:42-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1525591517047_197341542","id":"20180502-161025_1406483845","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T20:17:47-0400","dateFinished":"2018-05-06T20:17:47-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5790"},{"text":"%pyspark\nprint(\"%html <h3>Filter out empty/Len < 2 records<h3>\")","user":"anonymous","dateUpdated":"2018-05-06T03:33:40-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Filter out empty/Len < 2 records<h3>\n"}]},"apps":[],"jobName":"paragraph_1525591517047_197341542","id":"20180503-021548_48996307","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:34:23-0400","dateFinished":"2018-05-06T03:34:23-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5791"},{"text":"%pyspark\nproduction_df2 = production_df2.where(size(col(\"bigrams\")) >= 2)\n\nproduction_df2.na.drop()\nproduction_df2.count()","user":"anonymous","dateUpdated":"2018-05-06T20:17:45-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"997\n"}]},"apps":[],"jobName":"paragraph_1525591517047_197341542","id":"20180502-170236_1067380353","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T20:17:47-0400","dateFinished":"2018-05-06T20:17:51-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5792"},{"text":"%pyspark\nprint(\"%html <h3>Label mood column<h3>\")","user":"anonymous","dateUpdated":"2018-05-06T03:33:40-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Label mood column<h3>\n"}]},"apps":[],"jobName":"paragraph_1525591517047_197341542","id":"20180503-021614_2049105036","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:34:23-0400","dateFinished":"2018-05-06T03:34:29-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5793"},{"text":"%pyspark\n#String Indexer\nsi = StringIndexer(inputCol=\"mood\", outputCol=\"label\")\n\nsi_model = si.fit(production_df2)\nproduction_df3 = si_model.transform(production_df2)","user":"anonymous","dateUpdated":"2018-05-06T20:17:49-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1525591517048_195417798","id":"20180502-172607_393231834","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T20:17:49-0400","dateFinished":"2018-05-06T20:17:53-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5794"},{"text":"%pyspark\nprint(\"%html <h3>Count Vectorizer<h3>\")","user":"anonymous","dateUpdated":"2018-05-06T03:33:40-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Count Vectorizer<h3>\n"}]},"apps":[],"jobName":"paragraph_1525591517048_195417798","id":"20180503-021636_927669790","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:34:29-0400","dateFinished":"2018-05-06T03:34:31-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5795"},{"text":"%pyspark\ncv_1 = CountVectorizer(inputCol=\"unigrams\", outputCol=\"feature_1\")\n#cv_model_1 = cv_1.fit(production_df3)\n#cv_data_1 = cv_model_1.transform(production_df3)\n\n\ncv_2 = CountVectorizer(inputCol=\"bigrams\", outputCol=\"feature_2\")\n#cv_model_2 = cv_2.fit(production_df3)\n#cv_data_2 = cv_model_2.transform(cv_data_1)","user":"anonymous","dateUpdated":"2018-05-06T20:17:58-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1525591517048_195417798","id":"20180502-173002_829041103","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T20:17:58-0400","dateFinished":"2018-05-06T20:17:58-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5796"},{"text":"%pyspark\nprint(\"%html <h3>Vector Assembler<h3>\")","user":"anonymous","dateUpdated":"2018-05-06T03:33:40-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Vector Assembler<h3>\n"}]},"apps":[],"jobName":"paragraph_1525591517048_195417798","id":"20180503-184655_1855227195","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:34:31-0400","dateFinished":"2018-05-06T03:34:31-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5797"},{"text":"%pyspark\nva = VectorAssembler(inputCols=[\"feature_1\", \"feature_2\"], outputCol=\"features\")","user":"anonymous","dateUpdated":"2018-05-06T20:18:05-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1525591517048_195417798","id":"20180503-184710_1498991225","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T20:18:05-0400","dateFinished":"2018-05-06T20:18:05-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5798"},{"text":"%pyspark\nprint(\"%html <h3>TFIDF<h3>\")","user":"anonymous","dateUpdated":"2018-05-06T03:33:40-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>TFIDF<h3>\n"}]},"apps":[],"jobName":"paragraph_1525591517049_195033049","id":"20180503-021656_271842128","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:34:31-0400","dateFinished":"2018-05-06T03:34:31-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5799"},{"text":"%pyspark\nhashingTF_1 = HashingTF(inputCol=\"unigrams\", outputCol=\"feature_1\", numFeatures=500)\n#featurizedData_1 = hashingTF.transform(production_df3)\n\nhashingTF_2 = HashingTF(inputCol=\"bigrams\", outputCol=\"feature_2\", numFeatures=500)\n#featurizedData_2 = hashingTF.transform(featurizedData_1)","user":"anonymous","dateUpdated":"2018-05-06T20:18:13-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1525591517049_195033049","id":"20180503-021706_1946131645","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T20:18:13-0400","dateFinished":"2018-05-06T20:18:13-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5800"},{"text":"%pyspark\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = production_df5.randomSplit([0.7, 0.3])\ntrainingData.show()","user":"anonymous","dateUpdated":"2018-05-06T03:33:40-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2410588019121282840.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2410588019121282840.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'production_df5' is not defined\n\n"}]},"apps":[],"jobName":"paragraph_1525591517049_195033049","id":"20180502-175006_1009080958","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:34:31-0400","dateFinished":"2018-05-06T03:34:31-0400","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:5801"},{"text":"%pyspark\nprint(\"%html <H3>SVM</H3>\")","user":"anonymous","dateUpdated":"2018-05-06T03:33:40-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<H3>SVM</H3>\n"}]},"apps":[],"jobName":"paragraph_1525591517049_195033049","id":"20180503-011423_1300835939","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:34:31-0400","dateFinished":"2018-05-06T03:34:31-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5802"},{"text":"%pyspark\nsvm = LinearSVC(maxIter=10, regParam=0.01, featuresCol=\"feature_1\", labelCol=\"label\")\nsvm_model = svm.fit(trainingData)","user":"anonymous","dateUpdated":"2018-05-06T03:33:40-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2410588019121282840.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2410588019121282840.py\", line 360, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 2, in <module>\nNameError: name 'trainingData' is not defined\n\n"}]},"apps":[],"jobName":"paragraph_1525591517049_195033049","id":"20180502-173644_371821063","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:34:31-0400","dateFinished":"2018-05-06T03:34:31-0400","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:5803"},{"text":"%pyspark\npredictions = svm_model.transform(testData)\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(accuracy)","user":"anonymous","dateUpdated":"2018-05-06T03:33:40-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2410588019121282840.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2410588019121282840.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'svm_model' is not defined\n\n"}]},"apps":[],"jobName":"paragraph_1525591517050_196187295","id":"20180502-174722_1806406095","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:34:31-0400","dateFinished":"2018-05-06T03:34:31-0400","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:5804"},{"text":"%pyspark\nprint(\"%html <H3>NN</H3>\")","user":"anonymous","dateUpdated":"2018-05-06T03:33:40-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<H3>NN</H3>\n"}]},"apps":[],"jobName":"paragraph_1525591517050_196187295","id":"20180503-020159_1836935705","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:34:31-0400","dateFinished":"2018-05-06T03:34:31-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5805"},{"text":"%pyspark\n# specify layers for the neural network:\n# input layer of size 4 (features), two intermediate of size 5 and 4\n# and output of size 3 (classes)\nlayers = [4, 5, 4, 3]\n\n# create the trainer and set its parameters\ntrainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234, featuresCol=\"feature_1\")\n\n# train the model\nnn_model = trainer.fit(trainingData)\n","user":"anonymous","dateUpdated":"2018-05-06T03:33:40-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2410588019121282840.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2410588019121282840.py\", line 360, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 3, in <module>\nNameError: name 'trainingData' is not defined\n\n"}]},"apps":[],"jobName":"paragraph_1525591517050_196187295","id":"20180502-223712_1136591218","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:34:31-0400","dateFinished":"2018-05-06T03:34:31-0400","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:5806"},{"text":"%pyspark\n# compute accuracy on the test set\nresult1 = nn_model.transform(testData)\nresult1.show()\n#predictionAndLabels = result.select(\"prediction\", \"label\")\n#evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n#print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))","user":"anonymous","dateUpdated":"2018-05-06T03:33:41-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2410588019121282840.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2410588019121282840.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'nn_model' is not defined\n\n"}]},"apps":[],"jobName":"paragraph_1525591517050_196187295","id":"20180502-174718_633659489","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:34:31-0400","dateFinished":"2018-05-06T03:34:31-0400","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:5807"},{"text":"%pyspark\nprint(\"%html <H3>Naive Bayes Pipeline</H3>\")","user":"anonymous","dateUpdated":"2018-05-06T03:33:41-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<H3>Naive Bayes Pipeline</H3>\n"}]},"apps":[],"jobName":"paragraph_1525591517050_196187295","id":"20180503-020220_641468044","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:34:31-0400","dateFinished":"2018-05-06T03:34:31-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5808"},{"text":"%pyspark\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = production_df3.randomSplit([0.7, 0.3])\n\n\nnb1 = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", featuresCol=\"feature_1\")\n\n#Pipeline 1\npipeline_cv_nb_f1 = Pipeline(stages=[cv_1, cv_2, nb1])\nm1 = pipeline_cv_nb_f1.fit(trainingData)\npr1 = m1.transform(testData)\nevaluator = BinaryClassificationEvaluator()\nprint(\"Test set accuracy = \" + str(evaluator.evaluate(pr1)))\n\n\n\n#Pipeline 2\npipeline_tf_nb_f1 = Pipeline(stages=[hashingTF_1, hashingTF_2, nb1])\nm2 = pipeline_tf_nb_f1.fit(trainingData)\npr2 = m2.transform(testData)\nevaluator = BinaryClassificationEvaluator()\nprint(\"Test set accuracy = \" + str(evaluator.evaluate(pr2)))\n\n##############################################################################################################\nnb2 = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", featuresCol=\"feature_2\")\n\n#Pipeline 3\npipeline_cv_nb_f2 = Pipeline(stages=[cv_1, cv_2, nb2])\nm3 = pipeline_cv_nb_f2.fit(trainingData)\npr3 = m3.transform(testData)\nevaluator = BinaryClassificationEvaluator()\nprint(\"Test set accuracy = \" + str(evaluator.evaluate(pr3)))\n\n\n#Pipeline 4\npipeline_tf_nb_f2 = Pipeline(stages=[hashingTF_1, hashingTF_2, nb2])\nm4 = pipeline_tf_nb_f2.fit(trainingData)\n\npr4 = m4.transform(testData)\n\nevaluator = BinaryClassificationEvaluator()\nprint(\"Test set accuracy = \" + str(evaluator.evaluate(pr4)))\n\n##############################################################################################################\nnb3 = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", featuresCol=\"features\")\n\n#Pipeline 5\npipeline_cv_nb_f12 = Pipeline(stages=[cv_1, cv_2, va, nb3])\nm5 = pipeline_cv_nb_f12.fit(trainingData)\n\npr5 = m5.transform(testData)\n\nevaluator = BinaryClassificationEvaluator()\nprint(\"Test set accuracy = \" + str(evaluator.evaluate(pr5)))\n\n\n#Pipeline 6\npipeline_tf_nb_f12 = Pipeline(stages=[hashingTF_1, hashingTF_2, va, nb3])\nm6 = pipeline_tf_nb_f12.fit(trainingData)\n\npr6 = m6.transform(testData)\n\nevaluator = BinaryClassificationEvaluator()\nprint(\"Test set accuracy = \" + str(evaluator.evaluate(pr6)))","user":"anonymous","dateUpdated":"2018-05-06T20:20:02-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Test set accuracy = 0.498170843241\nTest set accuracy = 0.482760197549\nTest set accuracy = 0.419494238156\nTest set accuracy = 0.476038046461\nTest set accuracy = 0.454271081032\nTest set accuracy = 0.477547100787\n"}]},"apps":[],"jobName":"paragraph_1525591517051_195802546","id":"20180503-005414_567814209","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T20:20:02-0400","dateFinished":"2018-05-06T20:21:37-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5809"},{"text":"%pyspark\nprint(\"%html <H3>SVM Pipeline</H3>\")","user":"anonymous","dateUpdated":"2018-05-06T03:33:41-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<H3>SVM Pipeline</H3>\n"}]},"apps":[],"jobName":"paragraph_1525591517051_195802546","id":"20180503-024533_741117786","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:34:32-0400","dateFinished":"2018-05-06T03:36:01-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5810"},{"text":"%pyspark\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = production_df3.randomSplit([0.7, 0.3])\n\n\nsvm1 = LinearSVC(maxIter=10, regParam=0.01, featuresCol=\"feature_1\", labelCol=\"label\")\n\n#Pipeline 1\npipeline_cv_svm_f1 = Pipeline(stages=[cv_1, cv_2, svm1])\nm1 = pipeline_cv_svm_f1.fit(trainingData)\npr1 = m1.transform(testData)\nevaluator = BinaryClassificationEvaluator()\nprint(\"Test set accuracy = \" + str(evaluator.evaluate(pr1)))\n\n\n\n#Pipeline 2\npipeline_tf_svm_f1 = Pipeline(stages=[hashingTF_1, hashingTF_2, svm1])\nm2 = pipeline_tf_svm_f1.fit(trainingData)\npr2 = m2.transform(testData)\nevaluator = BinaryClassificationEvaluator()\nprint(\"Test set accuracy = \" + str(evaluator.evaluate(pr2)))\n\n##############################################################################################################\nsvm2 = LinearSVC(maxIter=10, regParam=0.01, featuresCol=\"feature_2\", labelCol=\"label\")\n\n#Pipeline 3\npipeline_cv_svm_f2 = Pipeline(stages=[cv_1, cv_2, svm2])\nm3 = pipeline_cv_svm_f2.fit(trainingData)\npr3 = m3.transform(testData)\nevaluator = BinaryClassificationEvaluator()\nprint(\"Test set accuracy = \" + str(evaluator.evaluate(pr3)))\n\n\n#Pipeline 4\npipeline_tf_svm_f2 = Pipeline(stages=[hashingTF_1, hashingTF_2, svm2])\nm4 = pipeline_tf_svm_f2.fit(trainingData)\npr4 = m4.transform(testData)\nevaluator = BinaryClassificationEvaluator()\nprint(\"Test set accuracy = \" + str(evaluator.evaluate(pr4)))\n\n##############################################################################################################\nsvm3 = LinearSVC(maxIter=10, regParam=0.01, featuresCol=\"features\", labelCol=\"label\")\n\n#Pipeline 5\npipeline_cv_svm_f12 = Pipeline(stages=[cv_1, cv_2, va, svm3])\nm5 = pipeline_cv_svm_f12.fit(trainingData)\npr5 = m5.transform(testData)\nevaluator = BinaryClassificationEvaluator()\nprint(\"Test set accuracy = \" + str(evaluator.evaluate(pr5)))\n\n\n#Pipeline 6\npipeline_tf_svm_f12 = Pipeline(stages=[hashingTF_1, hashingTF_2, va, svm3])\nm6 = pipeline_tf_svm_f12.fit(trainingData)\npr6 = m6.transform(testData)\nevaluator = BinaryClassificationEvaluator()\nprint(\"Test set accuracy = \" + str(evaluator.evaluate(pr6)))","user":"anonymous","dateUpdated":"2018-05-06T03:33:41-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Test set accuracy = 0.650207390649\nTest set accuracy = 0.634860482655\nTest set accuracy = 0.56427224736\nTest set accuracy = 0.452658371041\nTest set accuracy = 0.630788084465\nTest set accuracy = 0.59217571644\n"}]},"apps":[],"jobName":"paragraph_1525591517051_195802546","id":"20180503-194333_984783665","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:36:01-0400","dateFinished":"2018-05-06T03:43:49-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5811"},{"text":"%pyspark\nprint(\"%html <h3>SVM Hyperparameter Tuning</h3>\")\n","user":"anonymous","dateUpdated":"2018-05-06T03:33:41-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>SVM Hyperparameter Tuning</h3>\n"}]},"apps":[],"jobName":"paragraph_1525591517051_195802546","id":"20180504-001631_878235063","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:36:01-0400","dateFinished":"2018-05-06T03:43:49-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5812"},{"text":"%pyspark\n(trainingData, testData) = production_df3.randomSplit([0.7, 0.3])\n\n\nsvm1 = LinearSVC(maxIter=10, regParam=0.01, featuresCol=\"feature_1\", labelCol=\"label\")\n\n#Pipeline 1\npipeline_cv_svm_f1 = Pipeline(stages=[cv_1, cv_2, svm1])\nm1 = pipeline_cv_svm_f1.fit(trainingData)\ntestData = testData.drop(\"label\")\npr1 = m1.transform(testData)\npr1.show()\nevaluator = BinaryClassificationEvaluator()\nprint(\"Test set accuracy = \" + str(evaluator.evaluate(pr1)))\n\n","user":"anonymous","dateUpdated":"2018-05-06T03:33:41-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n|            unigrams| mood|             bigrams|           feature_1|           feature_2|       rawPrediction|prediction|\n+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n|[afraid, anyth, n...|  sad|[afraid anyth, an...|(7513,[0,1,3,11,1...|(52213,[15,184,70...|[0.82376210156849...|       0.0|\n|[aint, funni, tur...|  sad|[aint funni, funn...|(7513,[0,1,2,4,5,...|(52213,[0,4,9,59,...|[-0.3380126130455...|       1.0|\n|[anesthet, synthe...|  sad|[anesthet synthes...|(7513,[84,85,94,9...|(52213,[38660],[4...|[1.10519357664544...|       0.0|\n|[annabel, sevente...|happy|[annabel seventee...|(7513,[3,6,12,18,...|(52213,[64,568,72...|[0.00284019507401...|       0.0|\n|[antisocieti, soc...|  sad|[antisocieti soci...|(7513,[3,11,12,18...|(52213,[90,5725,2...|[0.57122135402581...|       0.0|\n|[background, fire...|happy|[background fire,...|(7513,[0,1,2,4,5,...|(52213,[0,19,73,1...|[1.01298938838986...|       0.0|\n|[bat, eye, girl, ...|  sad|[bat eye, eye gir...|(7513,[0,2,4,5,6,...|(52213,[380,928,1...|[0.32828646937156...|       0.0|\n|[believ, one, lov...|happy|[believ one, one ...|(7513,[2,6,8,9,17...|(52213,[26,160,61...|[-0.7565799742972...|       1.0|\n|[beyondyour, sunb...|  sad|[beyondyour sunbe...|(7513,[39,49,69,7...|(52213,[14445],[1...|[2.26858172294300...|       0.0|\n|[black, wing, suk...|  sad|[black wing, wing...|(7513,[21,28,35,8...|(52213,[684,1538,...|[0.44981606208432...|       0.0|\n|[bless, assur, je...|happy|[bless assur, ass...|(7513,[2,18,39,53...|(52213,[1439,1137...|[-3.5612425838735...|       1.0|\n|[blue, blue, day,...|happy|[blue blue, blue ...|(7513,[0,2,5,13,1...|(52213,[23,490,55...|[-0.1712135323532...|       1.0|\n|[blue, lightgreen...|  sad|[blue lightgreen,...|(7513,[13,54,55,6...|(52213,[46003],[1...|[0.12627652650252...|       0.0|\n|[born, mother, wo...|  sad|[born mother, mot...|(7513,[5,8,19,35,...|(52213,[1363,7016...|[1.05715840810775...|       0.0|\n|[break, dawn, tim...|  sad|[break dawn, dawn...|(7513,[0,1,3,4,5,...|(52213,[0,5,7,62,...|[0.10871657661043...|       0.0|\n|[breath, without,...|  sad|[breath without, ...|(7513,[6,35,55,70...|(52213,[8436,1011...|[0.22821255764867...|       0.0|\n|[broke, hous, las...|  sad|[broke hous, hous...|(7513,[1,2,8,21,2...|(52213,[638,660,1...|[0.46442064039041...|       0.0|\n|[buri, song, rhyt...|  sad|[buri song, song ...|(7513,[0,1,5,8,11...|(52213,[0,31,1334...|[-0.4219321212595...|       1.0|\n|[busta, rhyme, ge...|  sad|[busta rhyme, rhy...|(7513,[0,1,2,3,4,...|(52213,[7,22,58,6...|[1.21969262548605...|       0.0|\n|[call, name, thin...|  sad|[call name, name ...|(7513,[0,1,2,3,4,...|(52213,[7,9,12,14...|[0.80537838721548...|       0.0|\n+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n"},{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2410588019121282840.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2410588019121282840.py\", line 360, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 9, in <module>\n  File \"/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/pyspark/ml/evaluation.py\", line 69, in evaluate\n    return self._evaluate(dataset)\n  File \"/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/pyspark/ml/evaluation.py\", line 99, in _evaluate\n    return self._java_obj.evaluate(dataset._jdf)\n  File \"/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/utils.py\", line 79, in deco\n    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)\nIllegalArgumentException: u'Field \"label\" does not exist.'\n\n"}]},"apps":[],"jobName":"paragraph_1525591517051_195802546","id":"20180504-010610_1371862396","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:43:50-0400","dateFinished":"2018-05-06T03:45:08-0400","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:5813"},{"text":"%pyspark\ncrossval = CrossValidator(estimator=pipeline_cv_svm_f1,\n                          estimatorParamMaps=paramGrid1,\n                          evaluator=BinaryClassificationEvaluator(),\n                          numFolds=8)\ncvModel = crossval.fit(trainingData)\nprediction = cvModel.transform(test)\n","user":"anonymous","dateUpdated":"2018-05-06T03:33:41-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2410588019121282840.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2410588019121282840.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 2, in <module>\nNameError: name 'paramGrid1' is not defined\n\n"}]},"apps":[],"jobName":"paragraph_1525591517052_193878802","id":"20180504-013806_2028513034","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:43:50-0400","dateFinished":"2018-05-06T03:45:08-0400","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:5814"},{"text":"%pyspark\ntrainingData.show(2)\ntestData.show(2)\npr1.show(2)","user":"anonymous","dateUpdated":"2018-05-06T03:33:41-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+-----+--------------------+-----+\n|            unigrams| mood|             bigrams|label|\n+--------------------+-----+--------------------+-----+\n|[1st, vers, hey, ...|happy|[1st vers, vers h...|  1.0|\n|[accid, arm, that...|happy|[accid arm, arm t...|  1.0|\n+--------------------+-----+--------------------+-----+\nonly showing top 2 rows\n\n+--------------------+----+--------------------+\n|            unigrams|mood|             bigrams|\n+--------------------+----+--------------------+\n|[afraid, anyth, n...| sad|[afraid anyth, an...|\n|[aint, funni, tur...| sad|[aint funni, funn...|\n+--------------------+----+--------------------+\nonly showing top 2 rows\n\n+--------------------+----+--------------------+--------------------+--------------------+--------------------+----------+\n|            unigrams|mood|             bigrams|           feature_1|           feature_2|       rawPrediction|prediction|\n+--------------------+----+--------------------+--------------------+--------------------+--------------------+----------+\n|[afraid, anyth, n...| sad|[afraid anyth, an...|(7513,[0,1,3,11,1...|(52213,[15,184,70...|[0.82376210156849...|       0.0|\n|[aint, funni, tur...| sad|[aint funni, funn...|(7513,[0,1,2,4,5,...|(52213,[0,4,9,59,...|[-0.3380126130455...|       1.0|\n+--------------------+----+--------------------+--------------------+--------------------+--------------------+----------+\nonly showing top 2 rows\n\n"}]},"apps":[],"jobName":"paragraph_1525591517052_193878802","id":"20180504-023000_1983532810","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T03:45:08-0400","dateFinished":"2018-05-06T03:45:17-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5815"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2018-05-06T03:33:41-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1525591517052_193878802","id":"20180504-031003_998946884","dateCreated":"2018-05-06T03:25:17-0400","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5816"},{"text":"%pyspark\n(trainingData, testData) = production_df3.randomSplit([0.7, 0.3])\nsvm1 = LinearSVC(maxIter=10, regParam=0.01, featuresCol=\"feature_1\", labelCol=\"label\")\n#Pipeline 1\npipeline_cv_svm_f1 = Pipeline(stages=[cv_1, cv_2, svm1])\n\nparamGrid1 = ParamGridBuilder().addGrid(svm1.maxIter, [10]).build()\ncrossval = CrossValidator(estimator=pipeline_cv_svm_f1,\n                          estimatorParamMaps=paramGrid1,\n                          evaluator=BinaryClassificationEvaluator(),\n                          numFolds=8)\ncvModel = crossval.fit(trainingData)\nprediction = cvModel.transform(testData)\nprint(evaluator.evaluate(prediction))","user":"anonymous","dateUpdated":"2018-05-06T04:17:56-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"0.70945083014\n"}]},"apps":[],"jobName":"paragraph_1525591517052_193878802","id":"20180504-014424_1272617348","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T04:17:57-0400","dateFinished":"2018-05-06T04:30:03-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5817"},{"text":"%pyspark\nfrom pyspark.mllib.recommendation import MatrixFactorizationModel\n\n# Save and load model\nnb1.save(sc)\n#same_model = MatrixFactorizationModel.load(sc, model_path)","user":"anonymous","dateUpdated":"2018-05-06T06:40:10-0400","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2410588019121282840.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2410588019121282840.py\", line 360, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 2, in <module>\n  File \"/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/pyspark/ml/util.py\", line 144, in save\n    self.write().save(path)\n  File \"/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/pyspark/ml/util.py\", line 106, in save\n    raise TypeError(\"path should be a basestring, got type %s\" % type(path))\nTypeError: path should be a basestring, got type <class 'pyspark.context.SparkContext'>\n\n"}]},"apps":[],"jobName":"paragraph_1525591517052_193878802","id":"20180504-020902_295249433","dateCreated":"2018-05-06T03:25:17-0400","dateStarted":"2018-05-06T06:40:10-0400","dateFinished":"2018-05-06T06:40:10-0400","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:5818"},{"text":"%pyspark\nfrom pyspark import SparkContext\nfrom pyspark.mllib.classification import SVMWithSGD, SVMModel\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib import linalg as mllib_linalg\nfrom pyspark.ml import linalg as ml_linalg\n\ndef as_old(v):\n    if isinstance(v, ml_linalg.SparseVector):\n        return mllib_linalg.SparseVector(v.size, v.indices, v.values)\n    if isinstance(v, ml_linalg.DenseVector):\n        return mllib_linalg.DenseVector(v.values)\n    raise ValueError(\"Unsupported type {0}\".format(type(v)))\n\n\n#df3 = production_df3.rdd.map(tuple)\n\n\ndef parsePoint(line):\n    values = [float(x) for x in line.split(' ')]\n    return LabeledPoint(values[0], values[1:])\n\nparsedData = production_df3.rdd.map(parsePoint)\n\nmodel = SVMWithSGD.train(parsedData, iterations=100)\n\n# Evaluating the model on training data\n#labelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.feature_1)))\n#trainErr = labelsAndPreds.filter(lambda lp: lp[0] != lp[1]).count() / float(parsedData.count())\n#print(\"Training Error = \" + str(trainErr))\n\n    # Save and load model\n    #model.save(sc, \"target/tmp/pythonSVMWithSGDModel\")\n","user":"anonymous","dateUpdated":"2018-05-06T21:19:11-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-6149525267848635969.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-6149525267848635969.py\", line 360, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 16, in <module>\n  File \"/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/pyspark/mllib/classification.py\", line 553, in train\n    return _regression_train_wrapper(train, SVMModel, data, initialWeights)\n  File \"/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/pyspark/mllib/regression.py\", line 208, in _regression_train_wrapper\n    first = data.first()\n  File \"/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 1361, in first\n    rs = self.take(1)\n  File \"/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 1343, in take\n    res = self.context.runJob(self, takeUpToNumLeft, p)\n  File \"/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/pyspark/context.py\", line 992, in runJob\n    port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n  File \"/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 122.0 failed 4 times, most recent failure: Lost task 0.3 in stage 122.0 (TID 155, compute-3-9.local, executor 12): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/pyspark/worker.py\", line 166, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/pyspark/worker.py\", line 55, in read_command\n    command = serializer._read_with_length(file)\n  File \"/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/pyspark/serializers.py\", line 454, in loads\n    return pickle.loads(obj)\n  File \"/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/pyspark/mllib/__init__.py\", line 28, in <module>\n    import numpy\n  File \"/share/apps/python/2.7.11/lib/python2.7/site-packages/numpy/__init__.py\", line 180, in <module>\n    from . import add_newdocs\n  File \"/share/apps/python/2.7.11/lib/python2.7/site-packages/numpy/add_newdocs.py\", line 13, in <module>\n    from numpy.lib import add_newdoc\n  File \"/share/apps/python/2.7.11/lib/python2.7/site-packages/numpy/lib/__init__.py\", line 17, in <module>\n    from . import scimath as emath\nImportError: cannot import name scimath\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/pyspark/worker.py\", line 166, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/pyspark/worker.py\", line 55, in read_command\n    command = serializer._read_with_length(file)\n  File \"/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/pyspark/serializers.py\", line 454, in loads\n    return pickle.loads(obj)\n  File \"/share/apps/spark/spark-2.2.0-bin-hadoop2.6/python/pyspark/mllib/__init__.py\", line 28, in <module>\n    import numpy\n  File \"/share/apps/python/2.7.11/lib/python2.7/site-packages/numpy/__init__.py\", line 180, in <module>\n    from . import add_newdocs\n  File \"/share/apps/python/2.7.11/lib/python2.7/site-packages/numpy/add_newdocs.py\", line 13, in <module>\n    from numpy.lib import add_newdoc\n  File \"/share/apps/python/2.7.11/lib/python2.7/site-packages/numpy/lib/__init__.py\", line 17, in <module>\n    from . import scimath as emath\nImportError: cannot import name scimath\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n\n\n"}]},"apps":[],"jobName":"paragraph_1525591696581_1948562005","id":"20180506-032816_692563249","dateCreated":"2018-05-06T03:28:16-0400","dateStarted":"2018-05-06T21:19:11-0400","dateFinished":"2018-05-06T21:19:32-0400","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:5819"},{"text":"%pyspark\nproduction_df3.show()","user":"anonymous","dateUpdated":"2018-05-06T23:34:10-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+-----+--------------------+-----+\n|            unigrams| mood|             bigrams|label|\n+--------------------+-----+--------------------+-----+\n|[vers, one, alrig...|  sad|[vers one, one al...|  0.0|\n|[adam, antmarco, ...|happy|[adam antmarco, a...|  1.0|\n|[ive, eras, ive, ...|  sad|[ive eras, eras i...|  0.0|\n|[littl, darl, you...|happy|[littl darl, darl...|  1.0|\n|[lead, vocal, gre...|  sad|[lead vocal, voca...|  0.0|\n|[imagin, whole, w...|happy|[imagin whole, wh...|  1.0|\n|[first, let, say,...|happy|[first let, let s...|  1.0|\n|[jame, jame, hold...|happy|[jame jame, jame ...|  1.0|\n|[conclus, illus, ...|  sad|[conclus illus, i...|  0.0|\n|[aint, love, aint...|  sad|[aint love, love ...|  0.0|\n|[brick, brick, st...|  sad|[brick brick, bri...|  0.0|\n|[make, differ, ki...|  sad|[make differ, dif...|  0.0|\n|[angi, angi, clou...|  sad|[angi angi, angi ...|  0.0|\n|[day, day, night,...|happy|[day day, day nig...|  1.0|\n|[ever, make, back...|  sad|[ever make, make ...|  0.0|\n|[see, want, someh...|  sad|[see want, want s...|  0.0|\n|[choru, cheek, la...|happy|[choru cheek, che...|  1.0|\n|[rip, hold, tell,...|  sad|[rip hold, hold t...|  0.0|\n|[drive, home, chr...|happy|[drive home, home...|  1.0|\n|[degrad, dont, wa...|  sad|[degrad dont, don...|  0.0|\n+--------------------+-----+--------------------+-----+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1525653313743_-1360745429","id":"20180506-203513_1276801653","dateCreated":"2018-05-06T20:35:13-0400","dateStarted":"2018-05-06T23:34:10-0400","dateFinished":"2018-05-06T23:34:20-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5820"},{"text":"    %pyspark\n","user":"anonymous","dateUpdated":"2018-05-06T23:32:56-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1525663949296_1720305600","id":"20180506-233229_868688392","dateCreated":"2018-05-06T23:32:29-0400","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:5821"}],"name":"Music mood Analysis 3","id":"2DE5M5SBV","angularObjects":{"2DCWZHJC5:shared_process":[],"2DEF98BFZ:shared_process":[],"2DCFFUKPG:shared_process":[],"2DEZP8XN3:shared_process":[],"2DE33NXRX:shared_process":[],"2DFD2MFP3:shared_process":[],"2DC7S7ZMR:shared_process":[],"2DEHD813Q:shared_process":[],"2DEM6DA6Z:shared_process":[],"2DE6PN8KZ:shared_process":[],"2DE85AXCY:shared_process":[],"2DEV3RUXR:shared_process":[],"2DEY3C26E:shared_process":[],"2DDXG58BZ:shared_process":[],"2DCHSV9U7:shared_process":[],"2DBPQCHQG:shared_process":[],"2DDCV66P4:shared_process":[],"2DC9S1FQB:shared_process":[],"2DF5FM92C:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}