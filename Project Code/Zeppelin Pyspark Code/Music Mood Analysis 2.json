{"paragraphs":[{"text":"%pyspark\r\n\r\nfrom pyspark.sql.functions import col, lower, regexp_replace, split\r\nfrom pyspark.ml.recommendation import ALS\r\nfrom pyspark.ml.feature import *\r\nfrom pyspark.ml.classification import *\r\nfrom pyspark.ml.evaluation import * \r\nfrom pyspark.sql.functions import *\r\nfrom nltk.stem.porter import *\r\nfrom pyspark.sql.functions import col, lower, regexp_replace, split\r\nfrom pyspark.sql.types import *\r\nfrom pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\r\nfrom pyspark.mllib.regression import LabeledPoint\r\n\r\n\r\ndef clean_text_words(c):\r\n  c = lower(c)\r\n  c = regexp_replace(c, \"[^a-zA-Z0-9\\\\s]\", \" \")\r\n  return c\r\n\r\ndf = spark.read.csv(\"senticnet4-comma.csv\").select(clean_text_words(col(\"_c0\")).alias(\"words\"),col(\"_c2\").alias(\"rank\"))\r\n\r\ntokenizer_words = Tokenizer(inputCol=\"words\", outputCol=\"tokens\")\r\ntokenized_df = tokenizer_words.transform(df).select(\"tokens\", \"rank\")\r\n\r\n\r\nremover_words = StopWordsRemover()\r\nremover_words.setInputCol(\"tokens\")\r\nremover_words.setOutputCol(\"vector_no_stopw\")\r\nremoved_df = remover_words.transform(tokenized_df).select(\"vector_no_stopw\", \"rank\")\r\n\r\n\r\n","user":"anonymous","dateUpdated":"2018-05-05T03:06:11-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1525417236415_-236107614","id":"20180504-030036_891953030","dateCreated":"2018-05-04T03:00:36-0400","dateStarted":"2018-05-05T03:06:11-0400","dateFinished":"2018-05-05T03:06:11-0400","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1627"},{"text":"%pyspark\r\nstemmer = PorterStemmer()\r\n\r\ndef stem_words(in_vec):\r\n    out_vec = []\r\n    for t in in_vec:\r\n        t_stem = stemmer.stem(t)\r\n        if len(t_stem) > 2:\r\n            out_vec.append(t_stem)       \r\n    return out_vec\r\n    \r\n\r\nstemmer_udf_words = udf(lambda x: stem_words(x), ArrayType(StringType()))\r\n\r\n# Create new df with vectors containing the stemmed tokens \r\nstemmed_df = (\r\n    removed_df\r\n        .withColumn(\"vector_stemmed\", stemmer_udf_words(\"vector_no_stopw\"))\r\n        .withColumn(\"rank\", removed_df['rank'])\r\n        .select(\"vector_stemmed\", \"rank\")\r\n)\r\nstemmed_df = stemmed_df.where(size(col(\"vector_stemmed\")) > 1)\r\nstemmed_df.show()\r\n","user":"anonymous","dateUpdated":"2018-05-05T03:06:11-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----------------+-----+\n|   vector_stemmed| rank|\n+-----------------+-----+\n|  [littl, hungri]|0.252|\n|  [littl, specif]|0.079|\n|      [lot, book]|0.047|\n|    [lot, energi]|0.255|\n|       [lot, fat]|-0.51|\n|    [lot, flower]|0.055|\n|      [lot, food]|0.033|\n|       [lot, fun]|0.557|\n|     [lot, money]|0.044|\n|      [lot, nois]|-0.61|\n|     [lot, peopl]|0.036|\n|   [lot, practic]|0.584|\n|       [lot, sex]|0.858|\n|     [lot, space]|0.629|\n|    [lot, stress]|-0.14|\n|     [lot, studi]| -0.5|\n|      [lot, time]|0.635|\n|      [lot, work]|0.566|\n|    [aaa, hockey]|-0.03|\n|[abandon, person]|-0.79|\n+-----------------+-----+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1525417439729_-43273438","id":"20180504-030359_1782502210","dateCreated":"2018-05-04T03:03:59-0400","dateStarted":"2018-05-05T03:06:11-0400","dateFinished":"2018-05-05T03:06:16-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1628"},{"text":"%pyspark\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.ml.feature import *\nfrom pyspark.ml.classification import *\nfrom pyspark.ml.evaluation import * \nfrom pyspark.sql.functions import *\nfrom nltk.stem.porter import *\nfrom pyspark.sql.functions import col, lower, regexp_replace, split\nfrom pyspark.sql.types import *\nfrom pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\nfrom pyspark.mllib.regression import LabeledPoint\n\n\ndf = spark.read.csv(\"train_lyrics_1000.csv\", header='true', multiLine='true', escape=\"\\\"\")\ndf.na.drop()\n\n\ndf = df['lyrics','mood']\n\ndef clean_text(c):\n  c = lower(c)\n  c = regexp_replace(c, \"^rt \", \"\")\n  c = regexp_replace(c, \"(https?\\://)\\S+\", \"\")\n  c = regexp_replace(c, \"[^a-zA-Z0-9\\\\s]\", \"\")\n  return c\n  \nclean_text_df = df.select(clean_text(col(\"lyrics\")).alias(\"words\"), col(\"mood\"))\n\n\n\ntokenizer = Tokenizer(inputCol=\"words\", outputCol=\"tokens\")\nvector_df = tokenizer.transform(clean_text_df).select(\"tokens\", \"mood\")\n\nremover = StopWordsRemover()\n\n# Specify input/output columns\nremover.setInputCol(\"tokens\")\nremover.setOutputCol(\"vector_no_stopw\")\n\n# Transform existing dataframe with the StopWordsRemover\nvector_no_stopw_df = remover.transform(vector_df).select(\"vector_no_stopw\", \"mood\")\n\n\nstemmer = PorterStemmer()\n\ndef stem(in_vec):\n    out_vec = []\n    for t in in_vec:\n        t_stem = stemmer.stem(t)\n        if len(t_stem) > 2:\n            out_vec.append(t_stem)       \n    return out_vec\n    \n\nstemmer_udf = udf(lambda x: stem(x), ArrayType(StringType()))\n\n# Create new df with vectors containing the stemmed tokens \nvector_stemmed_df = (\n    vector_no_stopw_df\n        .withColumn(\"vector_stemmed\", stemmer_udf(\"vector_no_stopw\"))\n        .withColumn(\"mood\", vector_no_stopw_df['mood'])\n        .select(\"vector_stemmed\", \"mood\")\n)\n\n# Rename df and column for clarity\nproduction_df1 = vector_stemmed_df.select(col(\"vector_stemmed\").alias(\"unigrams\"), col(\"mood\"))\nproduction_df1.show()\n","user":"anonymous","dateUpdated":"2018-05-05T03:06:11-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+-----+\n|            unigrams| mood|\n+--------------------+-----+\n|[vers, one, alrig...|  sad|\n|[adam, antmarco, ...|happy|\n|[ive, eras, ive, ...|  sad|\n|[littl, darl, you...|happy|\n|[lead, vocal, gre...|  sad|\n|[imagin, whole, w...|happy|\n|[first, let, say,...|happy|\n|[jame, jame, hold...|happy|\n|[conclus, illus, ...|  sad|\n|[aint, love, aint...|  sad|\n|[brick, brick, st...|  sad|\n|[make, differ, ki...|  sad|\n|[angi, angi, clou...|  sad|\n|[day, day, night,...|happy|\n|[ever, make, back...|  sad|\n|[see, want, someh...|  sad|\n|[choru, cheek, la...|happy|\n|[rip, hold, tell,...|  sad|\n|[drive, home, chr...|happy|\n|[degrad, dont, wa...|  sad|\n+--------------------+-----+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1525426261929_-1188413536","id":"20180504-053101_282465084","dateCreated":"2018-05-04T05:31:01-0400","dateStarted":"2018-05-05T03:06:12-0400","dateFinished":"2018-05-05T03:06:18-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1629"},{"text":"%pyspark\nstemmed_df = stemmed_df\njoined = production_df1.crossJoin(stemmed_df)\njoined = joined.na.drop()\njoined_select = joined.select(\"unigrams\", \"vector_stemmed\", \"rank\")\n\n\ndef calculate_rank(x,y,z):\n  if len(list(set(y)-set(x))) == 0:\n    return float(z)\n  else:\n    return 0.0\n  \ndifferencer=udf(lambda x,y,z: calculate_rank(x,y,z), FloatType())\ndiff_table = joined_select.limit(1000000).withColumn('difference', differencer('unigrams', 'vector_stemmed', 'rank'))\ndiff_table = diff_table.na.drop()\ndiff_table = diff_table.filter(diff_table['difference'] != 0.0)\ndiff_table.show()\n\n","user":"anonymous","dateUpdated":"2018-05-05T03:06:11-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+---------------+-----+----------+\n|            unigrams| vector_stemmed| rank|difference|\n+--------------------+---------------+-----+----------+\n|[vers, one, alrig...|   [back, door]|0.281|     0.281|\n|[vers, one, alrig...|[beyond, doubt]|-0.68|     -0.68|\n|[vers, one, alrig...|   [call, back]|0.196|     0.196|\n|[vers, one, alrig...| [call, friend]|0.317|     0.317|\n|[vers, one, alrig...|   [call, girl]|-0.83|     -0.83|\n|[vers, one, alrig...|    [caus, ill]|-0.11|     -0.11|\n|[vers, one, alrig...|  [cute, littl]|0.049|     0.049|\n|[vers, one, alrig...|  [everi, year]|-0.57|     -0.57|\n|[vers, one, alrig...|  [front, back]|0.613|     0.613|\n|[vers, one, alrig...|  [front, door]|0.026|     0.026|\n|[vers, one, alrig...| [game, action]|0.067|     0.067|\n|[vers, one, alrig...|    [get, back]| -0.7|      -0.7|\n|[vers, one, alrig...|    [get, done]|-0.05|     -0.05|\n|[vers, one, alrig...|   [get, front]|-0.14|     -0.14|\n|[vers, one, alrig...|     [get, hit]| -0.7|      -0.7|\n|[vers, one, alrig...|    [get, know]|0.579|     0.579|\n|[vers, one, alrig...|   [get, laugh]|-0.14|     -0.14|\n|[vers, one, alrig...|    [get, lose]|-0.62|     -0.62|\n|[vers, one, alrig...|    [get, need]|-0.05|     -0.05|\n|[vers, one, alrig...|     [get, ill]|0.283|     0.283|\n+--------------------+---------------+-----+----------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1525427630827_681243357","id":"20180504-055350_1874810643","dateCreated":"2018-05-04T05:53:50-0400","dateStarted":"2018-05-05T03:06:17-0400","dateFinished":"2018-05-05T03:07:05-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1630"},{"text":"%pyspark\ngroup_table = diff_table.select(\"unigrams\",\"difference\").groupBy(\"unigrams\").sum(\"difference\")\ngroup_table.show()","user":"anonymous","dateUpdated":"2018-05-05T03:06:11-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------------+\n|            unigrams|     sum(difference)|\n+--------------------+--------------------+\n|[lay, open, like,...| -0.8340000901371241|\n|[brick, brick, st...|   7.894999945536256|\n|[cool, breez, aut...|   6.991000007838011|\n|[yeah, uhhuh, jum...|   3.368000065907836|\n|[oooooooo, oooooo...|  0.7369999811053276|\n|[want, love, want...|   4.559999972581863|\n|[radio, announc, ...|   5.765999961644411|\n|[see, want, someh...| -0.9259999841451645|\n|[lead, vocal, gre...|   4.318000175058842|\n|[choru, cheek, la...|  12.300000134855509|\n|[aint, love, aint...|   3.009000087156892|\n|[littl, darl, you...|   9.522999849170446|\n|[look, round, see...| -1.6520000249147415|\n|[day, day, night,...|-0.38899997249245644|\n|[drive, home, chr...|  4.9240000788122416|\n|[pharoah, monch, ...|  17.839999720454216|\n|[mari, cri, babi,...|-0.00699999928474...|\n|[darlin, cant, ex...|   3.861999996006489|\n|[imagin, whole, w...|  15.579999817535281|\n|[let, get, start,...|-0.35100007988512516|\n+--------------------+--------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1525464353594_1191237785","id":"20180504-160553_1117892734","dateCreated":"2018-05-04T16:05:53-0400","dateStarted":"2018-05-05T03:06:19-0400","dateFinished":"2018-05-05T03:07:50-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1631"},{"text":"%pyspark\ncomplete_table = production_df1.join(group_table,\"unigrams\")\ncomplete_table = complete_table.select(col(\"sum(difference)\").alias(\"sumDiff\"), col(\"mood\"))\n#complete_table = complete_table.filter(complete_table['sumDiff'] > 1)\ncomplete_table.show()","user":"anonymous","dateUpdated":"2018-05-05T03:06:11-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+-----+\n|             sumDiff| mood|\n+--------------------+-----+\n| -0.8340000901371241|  sad|\n|   7.894999945536256|  sad|\n|   6.991000007838011|  sad|\n|   3.368000065907836|happy|\n|   3.368000065907836|happy|\n|  0.7369999811053276|  sad|\n|   4.559999972581863|happy|\n|   5.765999961644411|happy|\n| -0.9259999841451645|  sad|\n|   4.318000175058842|  sad|\n|  12.300000134855509|happy|\n|   3.009000087156892|  sad|\n|   9.522999849170446|happy|\n| -1.6520000249147415|happy|\n|-0.38899997249245644|happy|\n|  4.9240000788122416|happy|\n|  17.839999720454216|  sad|\n|-0.00699999928474...|  sad|\n|   3.861999996006489|  sad|\n|  15.579999817535281|happy|\n+--------------------+-----+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1525466766483_-1803614837","id":"20180504-164606_1330329817","dateCreated":"2018-05-04T16:46:06-0400","dateStarted":"2018-05-05T03:07:06-0400","dateFinished":"2018-05-05T03:08:45-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1632"},{"text":"%pyspark\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.feature import VectorAssembler\n\n\nvecAssembler = VectorAssembler(inputCols= [\"sumDiff\"], outputCol=\"features\")\nvector_df = vecAssembler.transform(complete_table)\n\n#kmeans clustering\nkmeans=KMeans(k=2, seed=1)\nmodel=kmeans.fit(vector_df)\npredictions=model.transform(vector_df)\npredictions.show()","user":"anonymous","dateUpdated":"2018-05-05T03:46:57-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+-----+--------------------+----------+\n|             sumDiff| mood|            features|prediction|\n+--------------------+-----+--------------------+----------+\n| -0.8340000901371241|  sad|[-0.8340000901371...|         1|\n|   7.894999945536256|  sad| [7.894999945536256]|         0|\n|   6.991000007838011|  sad| [6.991000007838011]|         0|\n|   3.368000065907836|happy| [3.368000065907836]|         1|\n|   3.368000065907836|happy| [3.368000065907836]|         1|\n|  0.7369999811053276|  sad|[0.7369999811053276]|         1|\n|   4.559999972581863|happy| [4.559999972581863]|         1|\n|   5.765999961644411|happy| [5.765999961644411]|         0|\n| -0.9259999841451645|  sad|[-0.9259999841451...|         1|\n|   4.318000175058842|  sad| [4.318000175058842]|         1|\n|  12.300000134855509|happy|[12.300000134855509]|         0|\n|   3.009000087156892|  sad| [3.009000087156892]|         1|\n|   9.522999849170446|happy| [9.522999849170446]|         0|\n| -1.6520000249147415|happy|[-1.6520000249147...|         1|\n|-0.38899997249245644|happy|[-0.3889999724924...|         1|\n|  4.9240000788122416|happy|[4.9240000788122416]|         1|\n|  17.839999720454216|  sad|[17.839999720454216]|         0|\n|-0.00699999928474...|  sad|[-0.0069999992847...|         1|\n|   3.861999996006489|  sad| [3.861999996006489]|         1|\n|  15.579999817535281|happy|[15.579999817535281]|         0|\n+--------------------+-----+--------------------+----------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1525469216812_1361341945","id":"20180504-172656_314560553","dateCreated":"2018-05-04T17:26:56-0400","dateStarted":"2018-05-05T03:46:57-0400","dateFinished":"2018-05-05T03:49:43-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1633"},{"text":"%pyspark\nfrom pyspark.ml.feature import StringIndexer\n\nindexer = StringIndexer(inputCol=\"mood\", outputCol=\"mood_binary\")\nindexed = indexer.fit(predictions).transform(predictions)\nindexed = indexed.select(col(\"sumDiff\"), col(\"mood\"), col(\"features\"),col(\"prediction\").alias(\"predict\").cast(DoubleType()), col(\"mood_binary\").cast(IntegerType()))\nindexed.show()","user":"anonymous","dateUpdated":"2018-05-05T04:07:10-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+-----+--------------------+-------+-----------+\n|             sumDiff| mood|            features|predict|mood_binary|\n+--------------------+-----+--------------------+-------+-----------+\n| -0.8340000901371241|  sad|[-0.8340000901371...|    1.0|          0|\n|   7.894999945536256|  sad| [7.894999945536256]|    0.0|          0|\n|   6.991000007838011|  sad| [6.991000007838011]|    0.0|          0|\n|   3.368000065907836|happy| [3.368000065907836]|    1.0|          1|\n|   3.368000065907836|happy| [3.368000065907836]|    1.0|          1|\n|  0.7369999811053276|  sad|[0.7369999811053276]|    1.0|          0|\n|   4.559999972581863|happy| [4.559999972581863]|    1.0|          1|\n|   5.765999961644411|happy| [5.765999961644411]|    0.0|          1|\n| -0.9259999841451645|  sad|[-0.9259999841451...|    1.0|          0|\n|   4.318000175058842|  sad| [4.318000175058842]|    1.0|          0|\n|  12.300000134855509|happy|[12.300000134855509]|    0.0|          1|\n|   3.009000087156892|  sad| [3.009000087156892]|    1.0|          0|\n|   9.522999849170446|happy| [9.522999849170446]|    0.0|          1|\n| -1.6520000249147415|happy|[-1.6520000249147...|    1.0|          1|\n|-0.38899997249245644|happy|[-0.3889999724924...|    1.0|          1|\n|  4.9240000788122416|happy|[4.9240000788122416]|    1.0|          1|\n|  17.839999720454216|  sad|[17.839999720454216]|    0.0|          0|\n|-0.00699999928474...|  sad|[-0.0069999992847...|    1.0|          0|\n|   3.861999996006489|  sad| [3.861999996006489]|    1.0|          0|\n|  15.579999817535281|happy|[15.579999817535281]|    0.0|          1|\n+--------------------+-----+--------------------+-------+-----------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1525485238124_152014937","id":"20180504-215358_1247525308","dateCreated":"2018-05-04T21:53:58-0400","dateStarted":"2018-05-05T04:07:10-0400","dateFinished":"2018-05-05T04:13:22-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1634"},{"text":"%pyspark\ncorrect_rows = indexed.filter(indexed['predict'] == indexed['mood_binary'])\ncorrect = correct_rows.count()\nprint(correct)\ntotal = indexed.count()\nprint(total)\n#accuracy = correct / total * 100\n#print(accuracy)","user":"anonymous","dateUpdated":"2018-05-05T03:52:55-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"19\n42\n"}]},"apps":[],"jobName":"paragraph_1525499123247_33150403","id":"20180505-014523_225298142","dateCreated":"2018-05-05T01:45:23-0400","dateStarted":"2018-05-05T03:52:55-0400","dateFinished":"2018-05-05T03:55:59-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1635"},{"text":"%pyspark\n\n(trainingData, testData) = indexed.randomSplit([0.7, 0.3])\nsvm = LinearSVC(maxIter=10, regParam=0.01, featuresCol=\"features\", labelCol=\"mood_binary\")\nsvm_model = svm.fit(trainingData)\nsvm_predictions = svm_model.transform(testData)\n\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"mood_binary\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(svm_predictions)\nprint(accuracy)","user":"anonymous","dateUpdated":"2018-05-05T04:33:11-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"0.4\n"}]},"apps":[],"jobName":"paragraph_1525500867830_1584760042","id":"20180505-021427_414562280","dateCreated":"2018-05-05T02:14:27-0400","dateStarted":"2018-05-05T04:33:11-0400","dateFinished":"2018-05-05T04:40:20-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1636"},{"text":"%pyspark\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n\n(trainingData, testData) = indexed.randomSplit([0.7, 0.3])\nsvm = LinearSVC(maxIter=10, regParam=0.01, featuresCol=\"features\", labelCol=\"mood_binary\")\nsvm_model = svm.fit(trainingData)\nsvm_predictions = svm_model.transform(testData)\n\n# Evaluate model\nevaluator_binary = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"mood_binary\")\nprint(evaluator_binary.evaluate(svm_predictions))","user":"anonymous","dateUpdated":"2018-05-05T04:41:40-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"0.375\n"}]},"apps":[],"jobName":"paragraph_1525505528423_-1900528353","id":"20180505-033208_811039054","dateCreated":"2018-05-05T03:32:08-0400","dateStarted":"2018-05-05T04:41:40-0400","dateFinished":"2018-05-05T04:44:41-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1637"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2018-05-05T04:05:56-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1525507556150_2079993991","id":"20180505-040556_1466285691","dateCreated":"2018-05-05T04:05:56-0400","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:1638"}],"name":"Music Mood Analysis 2","id":"2DC7WG785","angularObjects":{"2DCWZHJC5:shared_process":[],"2DEF98BFZ:shared_process":[],"2DCFFUKPG:shared_process":[],"2DEZP8XN3:shared_process":[],"2DE33NXRX:shared_process":[],"2DFD2MFP3:shared_process":[],"2DC7S7ZMR:shared_process":[],"2DEHD813Q:shared_process":[],"2DEM6DA6Z:shared_process":[],"2DE6PN8KZ:shared_process":[],"2DE85AXCY:shared_process":[],"2DEV3RUXR:shared_process":[],"2DEY3C26E:shared_process":[],"2DDXG58BZ:shared_process":[],"2DCHSV9U7:shared_process":[],"2DBPQCHQG:shared_process":[],"2DDCV66P4:shared_process":[],"2DC9S1FQB:shared_process":[],"2DF5FM92C:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}